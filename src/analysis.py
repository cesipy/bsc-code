import time

import torch; from torch import nn; from torch.utils.data import DataLoader
import torch.nn.functional as F
from ckatorch.core import cka_batch, cka_base
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#the platonic hypothesis metrics
import metrics; import metrics_llmrepsim

from tqdm import tqdm


import cca_core
from vilbert import ViLBERT
from config import *
import utils
from logger import Logger
import measures

logger = Logger()

SCORES_VMAX = 1

def _create_similarity_heatmaps(
    cross_modal: np.ndarray,
    text_text: np.ndarray,
    vision_vision: np.ndarray,
    num_layers: int,
    title_prefix: str,
    cmap: str = 'magma',
    vmin: float = 0,
    vmax: float = 1,
    reverse_cmap: bool = False,
    dir_name: str = None,
    filename_extension: Optional[str] = None,
    filename_suffix: str = "matrices"
):
    """Unified function to create 1x3 similarity heatmaps. Generated by GenAI"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    cmap_name = f"{cmap}_r" if reverse_cmap else cmap
    im1 = axes[0].imshow(cross_modal, cmap=cmap_name, vmin=vmin, vmax=vmax, aspect='equal')
    axes[0].set_title(f'Cross-Modal {title_prefix}', fontsize=14, pad=20);axes[0].set_xlabel('Vision Layer');axes[0].set_ylabel('Text Layer')
    im2 = axes[1].imshow(text_text, cmap=cmap_name, vmin=vmin, vmax=vmax, aspect='equal')
    axes[1].set_title(f'Text {title_prefix}', fontsize=14, pad=20)
    axes[1].set_xlabel('Text Layer');axes[1].set_ylabel('Text Layer')
    im3 = axes[2].imshow(vision_vision, cmap=cmap_name, vmin=vmin, vmax=vmax, aspect='equal')
    axes[2].set_title(f'Vision {title_prefix}', fontsize=14, pad=20)
    axes[2].set_xlabel('Vision Layer'); axes[2].set_ylabel('Vision Layer')

    # Set ticks
    for ax in axes:
        ax.set_xticks(range(num_layers));ax.set_yticks(range(num_layers))
        ax.set_xticklabels(range(num_layers)); ax.set_yticklabels(range(num_layers))

    plt.colorbar(im1, ax=axes[0], shrink=0.8);plt.colorbar(im2, ax=axes[1], shrink=0.8);plt.colorbar(im3, ax=axes[2], shrink=0.8)
    plt.tight_layout()
    if dir_name is None:
        timestamp = int(time.time())
        filename = f"res/plots/{filename_suffix}_{timestamp}.png"
    else:
        filename = f"{dir_name}/{filename_extension}_{filename_suffix}.png"
    fig.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    logger.info(f"Saved {title_prefix} heatmaps to {filename}")
    plt.close(fig)
    return cross_modal, text_text, vision_vision


def _visualize_procrustes(
    measure_per_layer: dict,
    num_layers: int,
    k: int = 10,
    dir_name: str = None,
    filename_extension: Optional[str] = None
):
    """Creates and saves heatmaps for Orthogonal Procrustes."""
    procrustes_dist_cross_modal = np.zeros((num_layers, num_layers))
    procrustes_dist_text_text = np.zeros((num_layers, num_layers))
    procrustes_dist_vision_vision = np.zeros((num_layers, num_layers))

    for i in tqdm(range(num_layers), leave=False, desc="Computing Procrustes"):
        for j in range(num_layers):
            text_cls = measure_per_layer[i]["text_embeddings"]
            vision_cls = measure_per_layer[j]["vision_embeddings"]
            text_cls_i = measure_per_layer[i]["text_embeddings"]
            text_cls_j = measure_per_layer[j]["text_embeddings"]
            vision_cls_i = measure_per_layer[i]["vision_embeddings"]
            vision_cls_j = measure_per_layer[j]["vision_embeddings"]

            procrustes_dist_cross_modal[i, j] = metrics_llmrepsim.orthogonal_procrustes(R=text_cls, Rp=vision_cls)
            procrustes_dist_text_text[i, j] = metrics_llmrepsim.orthogonal_procrustes(R=text_cls_i, Rp=text_cls_j)
            procrustes_dist_vision_vision[i, j] = metrics_llmrepsim.orthogonal_procrustes(R=vision_cls_i, Rp=vision_cls_j)

    return _create_similarity_heatmaps(
        cross_modal=procrustes_dist_cross_modal,text_text=procrustes_dist_text_text,vision_vision=procrustes_dist_vision_vision,
        num_layers=num_layers,
        title_prefix="Procrustes Distance",cmap="magma",
        vmin=None, vmax=None,
        reverse_cmap=True,
        dir_name=dir_name,
        filename_extension=filename_extension,filename_suffix="procrustes_matrices"
    )

def _visualize_jaccard(
    measure_per_layer: dict,
    num_layers: int,
    k: int = 10,
    dir_name: str = None,
    filename_extension: Optional[str]=None
    ):
    jaccard_cross_modal = np.zeros((num_layers, num_layers))
    jaccard_text_text = np.zeros((num_layers, num_layers))
    jaccard_vision_vision = np.zeros((num_layers, num_layers))
    for i in tqdm(range(num_layers), leave=False, desc="computing neighborhood measures"):
        for j in range(num_layers):
            text_cls_i = F.normalize(measure_per_layer[i]["text_embeddings"], dim=-1)
            text_cls_j = F.normalize(measure_per_layer[j]["text_embeddings"], dim=-1)
            vision_cls_i = F.normalize(measure_per_layer[i]["vision_embeddings"], dim=-1)
            vision_cls_j = F.normalize(measure_per_layer[j]["vision_embeddings"], dim=-1)
            # Cross-modal (text layer i CLS vs vision layer j CLS)
            text_cls = F.normalize(measure_per_layer[i]["text_embeddings"], dim=-1)
            vision_cls = F.normalize(measure_per_layer[j]["vision_embeddings"], dim=-1)
            jaccard_cross_modal[i,j] = metrics_llmrepsim.jaccard_similarity(R=text_cls, Rp=vision_cls, k=k)
            jaccard_text_text[i,j] = metrics_llmrepsim.jaccard_similarity(
                R=text_cls_i, Rp=text_cls_j, k=k
            )
            jaccard_vision_vision[i,j] = metrics_llmrepsim.jaccard_similarity(
                R=vision_cls_i, Rp=vision_cls_j, k=k
            )

    return _create_similarity_heatmaps(
        cross_modal=jaccard_cross_modal,text_text=jaccard_text_text,vision_vision=jaccard_vision_vision,
        num_layers=num_layers,
        title_prefix=f"k-NN Jaccard (k={k})",
        cmap="magma",
        vmin=0,
        vmax=SCORES_VMAX,
        dir_name=dir_name,filename_extension=filename_extension,filename_suffix="jaccard_matrices"
    )

def _visualize_cka(
    measure_per_layer: dict,
    num_layers: int,
    dir_name: str = None,
    filename_extension: Optional[str]=None
    ):

    cross_modal_matrix = np.zeros((num_layers, num_layers))
    text_text_matrix = np.zeros((num_layers, num_layers))
    vision_vision_matrix = np.zeros((num_layers, num_layers))
    for i in tqdm(range(num_layers), leave=False, desc="computing cka matrix"):
        for j in range(num_layers):
            current_text = measure_per_layer[i]["text_embeddings"]
            current_vision = measure_per_layer[j]["vision_embeddings"]
            text_i = measure_per_layer[i]["text_embeddings"]
            text_j = measure_per_layer[j]["text_embeddings"]
            vision_i = measure_per_layer[i]["vision_embeddings"]
            vision_j = measure_per_layer[j]["vision_embeddings"]

            cross_modal_matrix[i,j] = metrics.AlignmentMetrics.cka(current_text, current_vision)
            text_text_matrix[i,j] = metrics.AlignmentMetrics.cka(text_i, text_j)
            vision_vision_matrix[i,j] = metrics.AlignmentMetrics.cka(vision_i, vision_j)
    return _create_similarity_heatmaps(
        cross_modal=cross_modal_matrix,text_text=text_text_matrix,
        vision_vision=vision_vision_matrix,
        num_layers=num_layers,
        title_prefix="CKA (Linear)",
        cmap="magma",vmin=0,vmax=SCORES_VMAX,
        reverse_cmap=False,
        dir_name=dir_name,filename_extension=filename_extension,filename_suffix="cka_matrices"
    )
def _visualize_mutual_knn(
    measure_per_layer: dict,
    num_layers: int,
    k: int = 10,
    dir_name: str = None,
    filename_extension: Optional[str]=None
    ):

    cross_modal_matrix = np.zeros((num_layers, num_layers))
    text_text_matrix = np.zeros((num_layers, num_layers))
    vision_vision_matrix = np.zeros((num_layers, num_layers))

    for i in tqdm(range(num_layers), leave=False, desc="computing mknn matrix"):
        for j in range(num_layers):
            text_cls = F.normalize(measure_per_layer[i]["text_embeddings"], dim=-1)
            vision_cls = F.normalize(measure_per_layer[j]["vision_embeddings"], dim=-1)
            text_cls_i = F.normalize(measure_per_layer[i]["text_embeddings"], dim=-1)
            text_cls_j = F.normalize(measure_per_layer[j]["text_embeddings"], dim=-1)
            vision_cls_i = F.normalize(measure_per_layer[i]["vision_embeddings"], dim=-1)
            vision_cls_j = F.normalize(measure_per_layer[j]["vision_embeddings"], dim=-1)
            cross_modal_matrix[i,j] = metrics.AlignmentMetrics.mutual_knn(text_cls, vision_cls, topk=k)
            text_text_matrix[i,j] = metrics.AlignmentMetrics.mutual_knn(text_cls_i, text_cls_j, topk=k)
            vision_vision_matrix[i,j] = metrics.AlignmentMetrics.mutual_knn(vision_cls_i, vision_cls_j, topk=k)

    return _create_similarity_heatmaps(
        cross_modal=cross_modal_matrix,text_text=text_text_matrix,vision_vision=vision_vision_matrix,
        num_layers=num_layers,
        title_prefix=f"Mutual k-NN (k={k})",
        cmap="magma",
        vmin=0,vmax=SCORES_VMAX,
        dir_name=dir_name,filename_extension=filename_extension,filename_suffix="mutual_knn_matrices"
    )

def _visualize_svcca(measures_per_layer:dict, num_layers:int, dir_name:str=None,
    filename_extension:Optional[str]=None):

    cross_modal_matrix = np.zeros((num_layers, num_layers))
    text_text_matrix = np.zeros((num_layers, num_layers))
    vision_vision_matrix = np.zeros((num_layers, num_layers))
    for i in tqdm(range(num_layers), leave=False, desc="computing svcca matrix"):
        for j in range(num_layers):
            current_text = measures_per_layer[i]["text_embeddings"]
            current_vision = measures_per_layer[j]["vision_embeddings"]
            text_i = measures_per_layer[i]["text_embeddings"]
            text_j = measures_per_layer[j]["text_embeddings"]
            vision_i = measures_per_layer[i]["vision_embeddings"]
            vision_j = measures_per_layer[j]["vision_embeddings"]

            cross_modal_matrix[i,j] = metrics.AlignmentMetrics.svcca(current_text, current_vision, cca_dim=10)
            text_text_matrix[i,j] = metrics.AlignmentMetrics.svcca(text_i, text_j, cca_dim=10)
            vision_vision_matrix[i,j] = metrics.AlignmentMetrics.svcca(vision_i, vision_j, cca_dim=10)
    return _create_similarity_heatmaps(
        cross_modal=cross_modal_matrix,text_text=text_text_matrix,
        vision_vision=vision_vision_matrix,
        num_layers=num_layers,
        title_prefix="SVCCA (10 Dims)",
        cmap="magma",vmin=0,vmax=SCORES_VMAX,
        dir_name=dir_name,filename_extension=filename_extension,filename_suffix="svcca_matrices")



def run_alignment_visualization(
    dataloader: DataLoader,
    model: ViLBERT,
    dir_name=None,
    filename_extension:Optional[str]=None
    ):
    if filename_extension:
        assert dir_name is not None

    device = "cuda" if torch.cuda.is_available() else "cpu"
    measures_per_layer_cls = get_alignment_data(dataloader=dataloader, model=model, device=device)

    if dir_name is None:
        _visualize_procrustes(measures_per_layer_cls, model.depth, k=KNN_K,)
        _visualize_jaccard(measures_per_layer_cls, model.depth, k=KNN_K)
        _visualize_cka(measures_per_layer_cls, model.depth)
        _visualize_mutual_knn(measures_per_layer_cls, model.depth, k=KNN_K)
        _visualize_svcca(measures_per_layer_cls, model.depth)
    else:
        _visualize_procrustes(measures_per_layer_cls, model.depth, k=KNN_K,
            dir_name=dir_name, filename_extension=filename_extension)
        _visualize_jaccard(measures_per_layer_cls, model.depth, k=KNN_K,
            dir_name=dir_name, filename_extension=filename_extension)
        _visualize_cka(measures_per_layer_cls, model.depth,
            dir_name=dir_name, filename_extension=filename_extension)
        _visualize_mutual_knn(measures_per_layer_cls, model.depth, k=KNN_K,
            dir_name=dir_name, filename_extension=filename_extension)
        _visualize_svcca(measures_per_layer_cls, model.depth,
            dir_name=dir_name, filename_extension=filename_extension)


def get_alignment_data(dataloader: DataLoader, model:ViLBERT, device="cuda" if torch.cuda.is_available() else "cpu"):
    model.eval()
    model = model.to(device)
    with torch.no_grad():
        torch.cuda.empty_cache()

        layers = {}



    for i in range(model.depth):
        layers[i] = {
            "text_embeddings": [],
            "vision_embeddings": [],
            "is_cross_attention": True,#i in model.cross_attention_layers,
            "layer": i
        }




    for i, batch in enumerate(dataloader):
        text = {k: v.squeeze(1).to(device) for k, v in batch["text"].items()}
        image = {k: v.squeeze(1).to(device) for k, v in batch["img"].items()}
        label = batch["label"].to(device)

        with torch.no_grad():
            text_embedding, image_embedding, intermediate_representations =model.forward(
                text_input_ids=text["input_ids"],
                text_attention_mask=text["attention_mask"],
                text_token_type_ids=text.get("token_type_ids", None),
                image_pixel_values=image["pixel_values"],
                image_attention_mask=image.get("attention_mask", None),
                save_intermediate_representations=True
            )

            tmp_txt = intermediate_representations[-1]["text_embedding"][:, 0, :]
            # print(tmp_txt.shape)
            # print(f"len intermed: {len(intermediate_representations)}")
            # print(f"shape of intermediates: {intermediate_representations[0]}")
            # print(f"{intermediate_representations[-1]['text_embedding'].shape}")
            # print(tmp_txt.shape, text_embedding.shape)

            cmp_shape_t = text_embedding.shape
            cmp_shape_v = image_embedding.shape

            assert torch.equal(text_embedding , intermediate_representations[-1]["text_embedding"][:, 0, :])
            assert torch.equal(image_embedding , intermediate_representations[-1]["vision_embedding"][:, 0, :])
            del text, image, text_embedding, image_embedding
            for entry in intermediate_representations:
                text_embedding = entry["text_embedding"].detach().cpu()   # [bs, dim]
                vision_embedding = entry["vision_embedding"].detach().cpu() # [bs, dim]
                layer = entry["layer"]

                text_cls =  text_embedding[:,0,:]   # [bs, dim]
                vision_cls = vision_embedding[:,0,:] # [bs, dim]
                # print(f"vision_cls shape: {vision_cls.shape}, text_cls shape: {text_cls.shape}")
                assert text_cls.shape == cmp_shape_t
                assert vision_cls.shape == cmp_shape_v

                layers[layer]["text_embeddings"].append(text_cls)
                layers[layer]["vision_embeddings"].append(vision_cls)

    for layer in layers.keys():
        layers[layer]["text_embeddings"] = torch.cat(layers[layer]["text_embeddings"], dim=0)
        layers[layer]["vision_embeddings"] = torch.cat(layers[layer]["vision_embeddings"], dim=0)

    return layers

def calculate_metrics_old(text_embeddings, vision_embeddings, knn_k):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    text_embeddings = text_embeddings.to(device)
    vision_embeddings = vision_embeddings.to(device)

    norm_t = F.normalize(text_embeddings, dim=-1)
    norm_v = F.normalize(vision_embeddings, dim=-1)

    # no need for them anymore
    mknn = 0.0
    rank_sim = 0.0
    cka_linear = 0.0
    svcca = 0.0
    procrustes_dist = 0.0
    rsa = 0.0

    linear_r2 = measures.linear_r2_alignment(
        X=text_embeddings,
        Y=vision_embeddings
    )


    return {
        "mknn": mknn,
        "rank_sim": rank_sim,
        "procrustes_dist": procrustes_dist,
        "cka_linear": cka_linear,
        "svcca": svcca,
        "linear_r2": linear_r2,
        "rsa": rsa
    }

def calculate_metrics(text_embeddings, vision_embeddings, knn_k):
    norm_t = F.normalize(text_embeddings, dim=-1)
    norm_v = F.normalize(vision_embeddings, dim=-1)

    cycle_knn = metrics.AlignmentMetrics.cycle_knn(feats_A=norm_t, feats_B=norm_v, topk=knn_k)
    mknn = metrics.AlignmentMetrics.measure("mutual_knn", norm_t, norm_v, topk=knn_k)
    cka = metrics.AlignmentMetrics.cka(text_embeddings, vision_embeddings)
    cka_rbf = metrics.AlignmentMetrics.cka(text_embeddings, vision_embeddings, kernel_metric="rbf")
    unbiased_cka = metrics.AlignmentMetrics.unbiased_cka(text_embeddings, vision_embeddings)
    svcca = metrics.AlignmentMetrics.svcca(text_embeddings, vision_embeddings, cca_dim=10)
    cknna = metrics.AlignmentMetrics.cknna(norm_t, norm_v, topk=knn_k)
    return {
        "cycle_knn": cycle_knn,
        "mknn": mknn,
        "cka": cka,
        "cka_rbf": cka_rbf,
        "unbiased_cka": unbiased_cka,
        "svcca": svcca,
        "cknna": cknna
    }

def get_additional_metrics(text_embeddings, vision_embeddings, knn_k):
    # really big differences
    procrustes = metrics_llmrepsim.orthogonal_procrustes(R=text_embeddings, Rp=vision_embeddings)

    # they are pretty similar,
    jaccard = metrics_llmrepsim.jaccard_similarity(R=text_embeddings, Rp=vision_embeddings, k=knn_k)

    # moderate diffs
    rsa= metrics_llmrepsim.representational_similarity_analysis(R=text_embeddings, Rp=vision_embeddings)
    return {
        "procrustes": procrustes,
        "jaccard": jaccard,
        "rsa": rsa
    }


def analyse_alignment(dataloader: DataLoader, model: ViLBERT,
        device = "cuda" if torch.cuda.is_available() else "cpu",
        knn_k=KNN_K):
    layers_data = get_alignment_data(dataloader=dataloader, model=model, device=device)

    metric_adds, metric_news, metric_olds = [],[],[]

    num_layers = len(layers_data.keys())
    for i in range(num_layers):
        data = layers_data[i]
        text_embeddings = data["text_embeddings"]
        vision_embeddings = data["vision_embeddings"]

        metrics_old = calculate_metrics_old(
            text_embeddings=text_embeddings,
            vision_embeddings=vision_embeddings,
            knn_k=knn_k)
        metrics_new = calculate_metrics(
            text_embeddings=text_embeddings,
            vision_embeddings=vision_embeddings,
            knn_k=knn_k
            )
        metrics_add = get_additional_metrics(text_embeddings=text_embeddings, vision_embeddings=vision_embeddings,
            knn_k=knn_k
        )

        metric_adds.append(metrics_add)
        metric_news.append(metrics_new)
        metric_olds.append(metrics_old)

        info_str = (f"layer {i:2}: mknn = {metrics_old['mknn']:4.2f}, "
               f"cka(lin) = {metrics_old['cka_linear']:5.2f}, "
               f"svcca = {metrics_old['svcca']:5.2f}, "
               f"rank-sim = {metrics_old['rank_sim']:5.2f}, "
               f"procrust = {metrics_old['procrustes_dist']:7.2f}, "
        )
        info_str2 = (f"layer {i:2}: mknn = {metrics_new['mknn']:4.2f}, "
                f"cka(lin)= {metrics_new['cka']:5.2f}, "
                f"svcca= {metrics_new['svcca']:4.2f}, "
                f"cka(rbf)= {metrics_new['cka_rbf']:5.2f}, "
                f"cka(unb)= {metrics_new['unbiased_cka']:5.2f}, "
                f"cknn-a= {metrics_new['cknna']:4.2f}, "
                f"cycleknn= {metrics_new['cycle_knn']:4.2f}")

        # print(info_str)
        print(info_str2)
        logger.info(info_str)
        logger.info(info_str2)

    return_dict = {}
    for i in range(num_layers):
        return_dict[i] = {
            "mknn": metric_news[i]["mknn"],
            "cka": metric_news[i]["cka"],
            "cka_rbf": metric_news[i]["cka_rbf"],
            "unbiased_cka": metric_news[i]["unbiased_cka"],
            "svcca": metric_news[i]["svcca"],
            "cknna": metric_news[i]["cknna"],
            "cycle_knn": metric_news[i]["cycle_knn"],
            "procrustes": metric_adds[i]["procrustes"],
            "jaccard": metric_adds[i]["jaccard"],
            "rsa": metric_adds[i]["rsa"],
            "r2": metric_olds[i]["linear_r2"],
        }
    return return_dict





def process_intermediate_repr(
    intermediate_reprs: list[dict],
    pooling_method:str ="cls"
    ):
    """
    processes intermediate representations and
    computes quantitative measurements.

    input: intermediate_reprs: list of dicts
    dict= {
        "layer": int,
        "text_embedding": torch.Tensor,     # shape [bs, num_tokens, dim]
        "vision_embedding": torch.Tensor,   # shape [bs, num_patches+1, dim]
        "is_cross_attention": bool
    }
    """
    assert pooling_method in ["cls", "mean"], "invalid pooling method specified!!"

    layers_sims = []

    for i, representation in enumerate(intermediate_reprs):
        # print(f"shape text: {representation['text_embedding'].shape}, shape image: {representation['vision_embedding'].shape}")

        # logger.info(f"dim before calculating cka: {representation['text_embedding'].shape}, {representation['vision_embedding'].shape}")
        cka_sim = measures.cka(
            # text_embedding=torch.nn.functional.normalize(representation["text_embedding"], dim=-1),
            # vision_embedding=torch.nn.functional.normalize(representation["vision_embedding"], dim=-1)
            text_embedding=representation["text_embedding"],
            vision_embedding=representation["vision_embedding"]
        )

        max_similarity_tp = measures.max_similarity_token_patch(
            text_embedding=representation["text_embedding"],
            vision_embedding=representation["vision_embedding"]
        )
        max_similarity_pt = measures.max_similarity_patch_token(
            text_embedding=representation["text_embedding"],
            vision_embedding=representation["vision_embedding"]
        )
        max_similarity_tp = max_similarity_tp.mean().item()
        max_similarity_pt = max_similarity_pt.mean().item()
        # print(f"temp value: {max_simil_avg}")

        # currently not working?
        # #TODO: FIX

        svcca_sim = 0.0
        svcca_sim = measures.svcca_similarity(
            # torch.nn.functional.normalize(representation["text_embedding"], dim=-1),
            # torch.nn.functional.normalize(representation["vision_embedding"], dim=-1)
            text_embedding=representation["text_embedding"],
            vision_embedding=representation["vision_embedding"]
        )

        # cka_sim = 0.0
        # max_similarity_tp = 0.0
        # max_similarity_pt = 0.0
        # svcca_sim = 0.0



        if pooling_method == "cls":
            text_embedding = representation["text_embedding"][:, 0, :]
            vision_embedding = representation["vision_embedding"][:, 0, :]

        elif pooling_method == "mean":
            text_embedding = torch.mean(representation["text_embedding"], dim=1)
            vision_embedding = torch.mean(representation["vision_embedding"], dim=1)

        #shape vision: [bs, 768]
        #shape text: [bs, 768]

        # TODO: currently only processing the first example in batch.
        # maybe something more sophisticated needed?

        text_embedding_sample = text_embedding
        vision_embedding_sample = vision_embedding
        is_corss_attention = representation["is_cross_attention"]
        layer = representation["layer"]


        cos_sim = measures.cosine_similarity_batch(
            text_embedding=text_embedding_sample,
            vision_embedding=vision_embedding_sample
        )

        rsa_sim = measures.rsa_similarity(
            X=text_embedding_sample,
            Y=vision_embedding_sample
        )

        linear_r2 = measures.linear_r2_alignment(
            X=text_embedding_sample,
            Y=vision_embedding_sample
        )




        layers_sims.append(
            {
                "layer": layer,
                "is_cross_attention": is_corss_attention,
                "cosine_similarity": torch.mean(cos_sim).item(),
                "cka_similarity": cka_sim,
                "max_similarity_tp": max_similarity_tp,
                "max_similarity_pt": max_similarity_pt,
                "svcca_similarity": svcca_sim,
                "rsa_similarity": rsa_sim,
                "linear_r2": linear_r2
            }
        )


        # print(f"Layer: {layer}, Cross Attention: {is_corss_attention}, "
            #   f"Cosine Similarity: {cos_sim}")
    return layers_sims




def analyse(
    layer_similarities: list[dict],
    num_layers: int,
    mknn_values:dict,
    rank_values:dict,
    procrustes_values:dict,
):
    """input format:
        {
            "layer": layer,
            "is_cross_attention": is_cross_attention,
            "cosine_similarity": float,
            "cka_similarity": float,
            "max_similarity": float
        }

        mknn_values:
        {
            "i": mknn-value,
        }

        same for the others

    """
    temp_dict = {}
    for i in range(num_layers):
        temp_dict[i] = {}

    layers = {}
    for i in range(num_layers):
        layers[f"layer{i}"] = {
            "is_cross_attention": False,
            "similarity_measures": [],
            "full_epoch_measures": mknn_values[i],
            "rank_measures": rank_values[i],
            "procrustes_measures": procrustes_values[i],
        }

    for similarity_measure in layer_similarities:
        layer = similarity_measure["layer"]
        is_cross_attention = similarity_measure["is_cross_attention"]


        layers[f"layer{layer}"]["similarity_measures"].append(similarity_measure)
        layers[f"layer{layer}"]["is_cross_attention"] = is_cross_attention

    for layer_name in layers:
        is_cross_attention = layers[layer_name]["is_cross_attention"]
        measures = layers[layer_name]["similarity_measures"]
        full_epoch_measure = layers[layer_name]["full_epoch_measures"]
        rank_measure = layers[layer_name]["rank_measures"]
        procrustes_measure = layers[layer_name]["procrustes_measures"]

        if measures:

            cos_values = [m["cosine_similarity"] for m in measures]
            cka_values = [m["cka_similarity"] for m in measures]
            max_similarity_values_tp = [m["max_similarity_tp"] for m in measures]
            max_similarity_values_pt = [m["max_similarity_pt"] for m in measures]
            svcca_values = [m["svcca_similarity"] for m in measures]
            linear_r2_values = [m["linear_r2"] for m in measures]
            rsa_values = [m["rsa_similarity"] for m in measures]



            avg_cosine = sum(cos_values) / len(cos_values)
            avg_cka = sum(cka_values) / len(cka_values)
            avg_max_similarity_tp = sum(max_similarity_values_tp) / len(max_similarity_values_tp)
            avg_max_similarity_pt = sum(max_similarity_values_pt) / len(max_similarity_values_pt)
            avg_svcca = sum(svcca_values) / len(svcca_values)
            avg_linear_r2 = sum(linear_r2_values) / len(linear_r2_values)
            avg_rsa = sum(rsa_values) / len(rsa_values)


            metrics = {
                "cosine": avg_cosine,
                "CKA": avg_cka,
                "max_sim_tp": avg_max_similarity_tp,
                "max_sim_pt": avg_max_similarity_pt,
                "SVCCA": avg_svcca,
                "mknn_full_epoch": full_epoch_measure,
                "rank_full_epoch": rank_measure,
                "procrustes_full_epoch": procrustes_measure,
                "linear_r2": avg_linear_r2,
                "rsa": avg_rsa
            }

            # info_str = f"layer {layer_name} (co-attn-{is_cross_attention}): " + \
            #         ", ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
            info_str = (
                f"layer {layer_name:7} (co-attn-{is_cross_attention:2}): "
                f"cosine={avg_cosine:7.3f}, "
                f"CKA={avg_cka:.3f}, "
                # f"max_sim_tp={avg_max_similarity_tp:.4f}, "
                # f"max_sim_pt={avg_max_similarity_pt:.4f}, "
                f"SVCCA={avg_svcca:.3f}, "
                f"mknn={full_epoch_measure:.3f}, "
                f"rank={rank_measure:.3f}, "
                f"procrustes={procrustes_measure:7.2f}",
                f"linear_r2={avg_linear_r2:5.2f}, "
                f"rsa={avg_rsa:5.2f}"

            )
            print(info_str)
            logger.info(info_str)

            layer_num = int(layer_name.replace("layer", ""))
            temp_dict[layer_num] = metrics

    return_dict = {}
    for layer in range(num_layers):
        return_dict[layer] = {
            "is_cross_attention": layers[f"layer{layer}"]["is_cross_attention"],
            "cosine": temp_dict[layer]["cosine"],
            "cka": temp_dict[layer]["CKA"],
            "max_sim_tp": temp_dict[layer]["max_sim_tp"],
            "max_sim_pt": temp_dict[layer]["max_sim_pt"],
            "svcca": temp_dict[layer]["SVCCA"],
            "mknn_full_epoch": temp_dict[layer]["mknn_full_epoch"],
            "rank_full_epoch": temp_dict[layer]["rank_full_epoch"],
            "procrustes_full_epoch": temp_dict[layer]["procrustes_full_epoch"]
        }
    return return_dict

def cka_custom(X, Y):
    """
    Compute Centered Kernel Alignment (CKA) between two matrices.

    Args:
        X: torch.Tensor of shape [n_samples, features_x]
        Y: torch.Tensor of shape [n_samples, features_y]

    Returns:
        CKA score (float between 0 and 1)
    """
    n = X.shape[0]

    # Compute linear Gram matrices
    K_X = torch.mm(X, X.T)  # [n, n]
    K_Y = torch.mm(Y, Y.T)  # [n, n]

    # Center the Gram matrices
    # H = I - (1/n) * ones_matrix
    ones = torch.ones(n, n, device=X.device)
    H = torch.eye(n, device=X.device) - (1/n) * ones

    K_X_centered = torch.mm(torch.mm(H, K_X), H)
    K_Y_centered = torch.mm(torch.mm(H, K_Y), H)

    # Compute CKA using Frobenius norm formula
    numerator = torch.trace(torch.mm(K_X_centered, K_Y_centered))

    norm_X = torch.norm(K_X_centered, p='fro')
    norm_Y = torch.norm(K_Y_centered, p='fro')
    denominator = norm_X * norm_Y

    if denominator == 0:
        return torch.tensor(0.0)

    cka_score = numerator / denominator
    return cka_score.item()



if __name__ == "__main__":
    # data1 = torch.rand( 250, 768)
    # data2 = torch.rand( 250,768)
    # start = time.time()
    # mknn = mutual_knn_alignment(Z1=data1, Z2=data2, k=5)
    # end = time.time()
    # diff_cpu = end - start

    # start = time.time()
    # mknn_gpu = mutual_knn_alignment_gpu(Z1=data1, Z2=data2, k=5)
    # end = time.time()
    # diff_gpu = end - start

    # print(f"mknn: {mknn}, mknn_gpu: {mknn_gpu}, diff = {mknn-mknn_gpu}")
    # print(f"cpu time: {diff_cpu}, gpu time: {diff_gpu}")

    # mknn_sim = mutual_knn_alignment(Z1=data1, Z2=data1, k=5)
    # mknn_sim_gpu = mutual_knn_alignment_gpu(Z1=data1, Z2=data1, k=5)
    # print(f"mknn identical: {mknn_sim}, mknn_gpu identical: {mknn_sim_gpu}, diff = {mknn_sim-mknn_sim_gpu}")

    data1 = torch.rand( 10000, 768)
    data2 = torch.rand( 10000,768)

    cka_identical = cka_base(
        x=data1,
        y=data1,
        kernel="linear",         # Use linear kernel (standard for CKA)
        unbiased=False,          # Use biased version (standard)
        method="fro_norm"        # Use Frobenius norm method
    )
    cka_different = cka_base(
        x=data1,
        y=data2,
        kernel="linear",         # Use linear kernel (standard for CKA)
        unbiased=False,          # Use biased version (standard)
        method="fro_norm"        # Use Frobenius norm method
    )
    print("shape:", data1.shape, data2.shape)
    print(f"cka identical: {cka_identical}, cka different: {cka_different}")
    # start = time.time()
    # mknn_gpu = mutual_knn_alignment_gpu_advanced(Z1=data1, Z2=data2, k=5)
    # end = time.time()
    # diff_gpu = end - start

    # print(f"mknn_gpu_advanced: {mknn_gpu}, time: {diff_gpu}")
    # mknn_simple = mutual_knn_alignment_simple(Z1=data1, Z2=data2, k=5)
    # print(f"mknn_simple: {mknn_simple}, diff = {mknn_gpu - mknn_simple}")

    # mknn_ident = mutual_knn_alignment_gpu_advanced(Z1=data1, Z2=data1, k=5)
    # print(f"mknn_gpu_advanced identical: {mknn_ident}")
    # mknn_simple_ident = mutual_knn_alignment_simple(Z1=data1, Z2=data1, k=5)
    # print(f"mknn_simple identical: {mknn_simple_ident}, diff = {mknn_ident - mknn_simple_ident}")

    # sim_identical =cka(data1, data1)
    # sim_different = cka(data1, data2)


    # print(f"sim indentical: {sim_identical}, sim different: {sim_different}")

    # svcca_sim_identical = svcca_similarity(
    #     text_embedding=data1,
    #     vision_embedding=data1
    # )
    # svcca_sim_different = svcca_similarity(
    #     text_embedding=data1,
    #     vision_embedding=data2
    # )
    # print(f"svcca sim identical: {svcca_sim_identical}, svcca sim different: {svcca_sim_different}")
